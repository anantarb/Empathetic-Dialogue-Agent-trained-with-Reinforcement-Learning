{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01_train_notebook.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"xZ8Xg8lA3pdh"},"source":["# Pre-training of GPT-2\n","\n","In this notebook we pre-train GPT-2 with the EmpathicDialog dataset to uses as a starting point for bringing in human feedback and to use as a baseline for comparision to our final model."]},{"cell_type":"code","metadata":{"id":"USX1kVz5VdUW"},"source":["%tensorflow_version 1.13.1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z4cSbFg93pd2"},"source":["Connect your own google drive to this notebook for saving the trained models:"]},{"cell_type":"code","metadata":{"id":"uEmQ8U4CWnA-"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"APqQ_rCNZWxd"},"source":["%cd /content/drive/My Drive/nlp-2021-vda/pre-training-gpt2\n","!pwd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tFOLXcu-3pd4"},"source":["Download the initial GPT-2 Model and datasets from Google Cloud storage:"]},{"cell_type":"code","metadata":{"id":"injQs43LULmP"},"source":["import os\n","if not os.path.isdir('./datasets'):\n","    !gsutil -m cp -r gs://nlp-lab/pre-training-gpt2/datasets ./\n","if not os.path.isdir('./gpt-2'):\n","    !gsutil -m cp -r gs://nlp-lab/pre-training-gpt2/gpt-2 ./"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xuDj25JbYazi"},"source":["Download our fine-tuned GPT-2 model, if you want to fine-tune on top. (uncomment lines below):"]},{"cell_type":"code","metadata":{"id":"KBaTe2Y8UTj9"},"source":["#!gsutil -m cp -r gs://nlp-lab/pre-training-gpt2/checkpoint ./"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L6XTMyY8aEb9"},"source":["from training import pre_train_gpt2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x-lJ5GmIa8L_"},"source":["sess = pre_train_gpt2.start_tf_sess()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nlttl3BmWH8e"},"source":["#Parameters Descriptions\n","\n","# sess - Tensorflow session\n","# train_dataset - path where the data is located\n","# val_dataset - path where the data is located\n","# steps - for how many steps do you wanna train the model (-1 means it will train infinitely\n","# model_name - Initial GPT model name i.e 124M or 335M or 775M\n","# model_dir - path where the initial GPT model is stored\n","# batch_size - Batch Size\n","# learning_rate - Learning Rate\n","# accumulate gradients - Accumulate gradients across N minibatches\n","# context_maxlen - maximum length of context tokens\n","# response_maxlen - maximum length of response tokens\n","# history_len - How many previous dialogues should be used in the context\n","# patience - how many steps do we wait for validation loss to go down\n","# restore_from - Either \"latest\", \"fresh\", or a path to a checkpoint file\n","# run_name - Run id. Name of subdirectory in checkpoint/\n","# checkpoint_dir - path where the checkpoints should be stored or located\n","# multi_gpu - set True, if you have multiple GPUs\n","# print_every - Print stats every N steps\n","# optimizer - which optimizer to use\n","# overwrite - Set true, if you wanna overwrite previous checkpoints\n","\n","pre_train_gpt2.finetune(sess=sess,\n","             train_dataset='datasets/empatheticdialogues/train.csv',\n","             val_dataset='datasets/empatheticdialogues/valid.csv',\n","             steps=-1,\n","             model_name='345M',\n","             model_dir='gpt-2/models',\n","             batch_size=8,\n","             learning_rate=0.000007,\n","             accumulate_gradients=5,\n","             context_maxlen=100,\n","             response_maxlen=100,\n","             history_len=4,\n","             patience=50,\n","             restore_from='latest',\n","             run_name='355M',\n","             checkpoint_dir='checkpoint',\n","             multi_gpu=False,\n","             print_every=1,\n","             max_checkpoints=1,\n","             optimizer='adam',\n","             overwrite=False)"],"execution_count":null,"outputs":[]}]}