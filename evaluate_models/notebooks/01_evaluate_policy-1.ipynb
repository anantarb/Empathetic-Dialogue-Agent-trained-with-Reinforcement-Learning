{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01_evaluate_policy-1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"SiAknrvzJ46N"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fa5VDP4GKEMn"},"source":["%cd /content/drive/MyDrive/nlp-2021-vda/evaluate_models/\n","!pwd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ITkGc7VUQjP-"},"source":["If you want to run evaluation on our sampled responses, download data from google cloud storage."]},{"cell_type":"code","metadata":{"id":"XokdRq_BQeSv"},"source":["import os\n","if not os.path.isdir('./sampled_responses'):\n","    !gsutil -m cp -r gs://nlp-lab/evaluate_models/sampled_responses ./"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HhsKkwyWbQS7"},"source":["if not os.path.isdir('./empathy_mental_health/trained_models'):\n","  !gsutil -m cp -r gs://nlp-lab/evaluate_models/empathy_mental_health/trained_models ./empathy_mental_health/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VjzvJm0MKF7C"},"source":["!pip install -U nltk"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F7pdkJk1v-nf"},"source":["!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BNoKsEjpwFbn"},"source":["When nltk.download opens choose 'Download' and as package 'stopwords'. Then choose quit."]},{"cell_type":"code","metadata":{"id":"3YmQiRmRKHdn"},"source":["import json\n","import nltk\n","nltk.download()\n","from nltk.translate.meteor_score import meteor_score\n","import matplotlib.pyplot as plt\n","import numpy as np\n","nltk.download('wordnet')\n","\n","# Metric calculation \n","from data import metrics\n","from data import parse_sampled_responses\n","from data import metric_averages_or_ratios\n","\n","# For plotting\n","import plotly.express as px\n","import plotly.graph_objs as go\n","import pandas as pd\n","from plotly.subplots import make_subplots"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U5pJoZO8Q8gA"},"source":["Plot METEOR Score vs Training Step and Returns vs Training Step"]},{"cell_type":"code","metadata":{"id":"oykLNMBAKL8G"},"source":["path_list = ['supervised_0.7', 'run7_1', 'run7_2', 'run7_3', 'run7_4', 'run7_5', 'run7_6', 'run7_7', 'run7_8']\n","file_scores = []\n","gold_path = 'sampled_responses/policy-1/gold.json'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NzNfpigkKORw"},"source":["for path in path_list:\n","  with open(gold_path) as f:\n","    gold = json.load(f)\n","\n","  with open(f\"sampled_responses/policy-1/{path}.json\") as f:\n","    data = json.load(f)\n","  scores = []\n","  for i in range(len(gold)):\n","    score = meteor_score([gold[i]['gold_response']], data[i]['sample0'])\n","    scores.append(score)\n","  file_scores.append(sum(scores) / len(scores))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FFUtmtqxKQgu"},"source":["x = [0, 25, 50, 75, 100, 125, 150, 175, 200]\n","returns = [0.99, 1.19, 1.28, 1.81, 2.2, 2.74, 3.08, 3.49, 3.57]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sv7mgy9gKRHj"},"source":["fig, ax1 = plt.subplots()\n","\n","color = 'tab:blue'\n","ax1.set_xlabel('Training step')\n","ax1.set_ylabel('METEOR score', color=color)\n","ax1.plot(x, file_scores, color=color)\n","ax1.tick_params(axis='y', labelcolor=color)\n","\n","ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n","\n","color = 'tab:orange'\n","ax2.set_ylabel('Returns', color=color)  # we already handled the x-label with ax1\n","ax2.plot(x, returns, color=color)\n","ax2.tick_params(axis='y', labelcolor=color)\n","\n","fig.tight_layout()  # otherwise the right y-label is slightly clipped\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"45oWsoC8vauG"},"source":["# Perplexity"]},{"cell_type":"markdown","metadata":{"id":"mTNH_-C2vca-"},"source":["# Utterance length"]},{"cell_type":"code","metadata":{"id":"z5MVotuMvhuf"},"source":["path_list = ['supervised_0.7', 'run7_1', 'run7_2', 'run7_3', 'run7_4', 'run7_5', 'run7_6', 'run7_7', 'run7_8']\n","x = [0, 25, 50, 75, 100, 125, 150, 175, 200]\n","gold_path = 'sampled_responses/policy-2/gold.json'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pRobc1i4vsIF"},"source":["def getWordCountMetricDictFromFilename(file_name, metric_dict):\n","    parsed_conversations = parse_sampled_responses.getParsedConversations(f\"sampled_responses/policy-1/{file_name}.json\",'sample0')\n","\n","    parsed_conversations_dict = metric_averages_or_ratios.getMetricDict(parsed_conversations,metric_dict,metric_dict)\n","    return parsed_conversations_dict\n","\n","def getGoldMetricDict(metric_dict):\n","    parsed_gold_conversations = parse_sampled_responses.getParsedConversations(gold_path,'gold_response')\n","    gold_metric = metric_averages_or_ratios.getMetricDict(parsed_gold_conversations,metric_dict,metric_dict)\n","    return gold_metric"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FFl5sWWzvvhk"},"source":["utterance_length_dict = {}\n","utterance_length_metric_names = ['utterance_length']\n","\n","gold_utterance_length = getGoldMetricDict(utterance_length_metric_names)\n","\n","counter = 0\n","for path in path_list:\n","    utterance_length = getWordCountMetricDictFromFilename(path,utterance_length_metric_names)\n","    utterance_length_dict[x[counter]] = utterance_length['utterance_length']\n","    counter = counter + 1\n","    \n","print(utterance_length_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d8aQJxo6wUBt"},"source":["fig = px.line(x=list(utterance_length_dict.keys()), y=list(utterance_length_dict.values()), title='Utternace length (y) vs #trainingsteps (x)')\n","fig.add_shape(go.layout.Shape(type=\"line\",\n","                                    name=\"gold\",\n","                                    x0=0,\n","                                    y0=gold_utterance_length['utterance_length'],\n","                                    x1=200,\n","                                    y1=gold_utterance_length['utterance_length'],\n","                                    line=dict(color='yellow', width=2),))\n","fig.append_trace(go.Scatter(\n","        showlegend = False,\n","        x=[210],\n","        y=[gold_utterance_length['utterance_length']],\n","        text=[\"gold\"],\n","        mode=\"text\",\n","    ),row=1,col=1)\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DSm7GoN5wWEi"},"source":["# Repetition"]},{"cell_type":"code","metadata":{"id":"Xu9y4XgYwYEQ"},"source":["repetition_dict = {}\n","\n","repetition_metric_names = ['conversation_repetition',\n","                            'self_repetition',\n","                            'utterance_repetition',\n","                            'word_repetition']\n","\n","gold_repetition_dict = getGoldMetricDict(repetition_metric_names)\n","\n","\n","counter = 0\n","for path in path_list:\n","    repetition_metrics = getWordCountMetricDictFromFilename(path,repetition_metric_names)\n","    repetition_dict[x[counter]] = repetition_metrics\n","    counter = counter + 1\n","print(repetition_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BicEpraMwb_5"},"source":["nrows = 4\n","df = pd.DataFrame(repetition_dict).transpose()\n","\n","plot_names = [i[0] for i in list(gold_repetition_dict.items())]\n","\n","fig = make_subplots(rows=nrows, cols=1, subplot_titles=plot_names)\n","\n","fig.update_xaxes(title_text=\"number of training steps\", row=nrows, col=1)\n","fig.update_yaxes(title_text=\"average word count\", row=2, col=1)\n","\n","\n","for i in range(0,nrows):\n","    fig.append_trace(go.Scatter(\n","        x=list(df.index),\n","        y=df.iloc[:,i],\n","        name=plot_names[i],\n","        legendgroup = '1',\n","    ), row=(i+1), col=1)\n","\n","\n","# add shapes\n","col_count = 1\n","for i in range(0,nrows):\n","    gold = list(gold_repetition_dict.items())[i][1]\n","    fig.add_shape(go.layout.Shape(type=\"line\",\n","                                    name=\"gold\",\n","                                    x0=0,\n","                                    y0=gold,\n","                                    x1=200,\n","                                    y1=gold,\n","                                    line=dict(color='yellow', width=2),),\n","                  row=(i+1),\n","                  col=1)\n","    fig.append_trace(go.Scatter(\n","        showlegend = False,\n","        x=[210],\n","        y=[gold],\n","        text=[\"gold\"],\n","        mode=\"text\",\n","    ), row=(i+1), col=1)\n","    col_count = col_count+1\n","\n","\n","fig.update_layout(height=600, width=800, title_text=\"Word Count metrics vs number of training steps\")\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2mq0o0WHweZA"},"source":["# Question"]},{"cell_type":"code","metadata":{"id":"6Zkr0R9VwgXl"},"source":["def getQuestionMetricDictFromFile(data_filepath,gold_filepath,response_type):\n","    parsed_conversations = parse_sampled_responses.getParsedConversations(data_filepath,response_type)\n","\n","    parsed_conversations_dict = metric_averages_or_ratios.getMetricDict(parsed_conversations,['question'], ['question'])\n","    \n","    if response_type == 'gold_response':\n","        parsed_conversations_dict[\"ratio_of_sample_is_question_of_all_samples\"] = 0\n","        parsed_conversations_dict[\"ratio_of_sample_is_question_if_gold_is_question\"] = 0\n","        parsed_conversations_dict[\"ratio_of_sample_is_question_if_gold_is_no_question\"] = 0\n","    else:\n","        parsed_conversations_gold = parse_sampled_responses.getParsedConversations(gold_filepath,'gold_response')\n","        parsed_conversations_dict[\"ratio_of_sample_is_question_of_all_samples\"] = metric_averages_or_ratios.getSampleQuestionOfAllSamplesRatio(parsed_conversations)\n","        parsed_conversations_dict[\"ratio_of_sample_is_question_if_gold_is_question\"] = metric_averages_or_ratios.getGoldQuestionVsSampleRatio(parsed_conversations_gold,parsed_conversations)\n","        parsed_conversations_dict[\"ratio_of_sample_is_question_if_gold_is_no_question\"] = metric_averages_or_ratios.getNoGoldQuestionVsSampleRatio(parsed_conversations_gold,parsed_conversations)\n","    \n","    return parsed_conversations_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kQxht-DSwiO1"},"source":["question_dict = {}\n","\n","gold_question_dict = getQuestionMetricDictFromFile(gold_path,gold_path,'gold_response')\n","\n","\n","counter = 0\n","for path in path_list:\n","    print(path)\n","    question_metrics = getQuestionMetricDictFromFile(f\"sampled_responses/policy-1/{path}.json\",gold_path,'sample0')\n","    question_dict[x[counter]] = question_metrics\n","    counter = counter + 1\n","print(question_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hwwvt7HewoCD"},"source":["nrows = 4\n","\n","df = pd.DataFrame(question_dict).transpose()\n","\n","plot_names = [i[0] for i in list(gold_question_dict.items())]\n","\n","fig = make_subplots(rows=nrows, cols=1, subplot_titles=plot_names)\n","\n","fig.update_xaxes(title_text=\"Training steps\", row=nrows, col=1)\n","fig.update_yaxes(title_text=\"value\", row=(2), col=1)\n","\n","\n","for i in range(0,nrows):\n","    fig.append_trace(go.Scatter(\n","        x=list(df.index),\n","        y=df.iloc[:,i],\n","        name=plot_names[i],\n","        legendgroup = '1',\n","    ), row=(i+1), col=1)\n","\n","\n","# add shapes\n","    \n","gold = list(gold_question_dict.items())[0][1]\n","fig.add_shape(go.layout.Shape(type=\"line\",\n","                                    name=\"gold\",\n","                                    x0=0,\n","                                    y0=gold,\n","                                    x1=200,\n","                                    y1=gold,\n","                                    line=dict(color='yellow', width=2),),\n","                  row=(1),\n","                  col=1)\n","fig.append_trace(go.Scatter(\n","    showlegend = False,\n","    x=[210],\n","    y=[gold],\n","    text=[\"gold\"],\n","    mode=\"text\",\n","), row=1, col=1)\n","\n","\n","fig.update_layout(height=600, width=1000, title_text=\"Word Count metrics vs number of training steps\")\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YKpmjLsMxHvM"},"source":["# Empathy"]},{"cell_type":"code","metadata":{"id":"JRQ_RRuAxI9n"},"source":["content_metric_names = ['empathy']\n","\n","content_metric_names_separated = [\n","                            'emotional_reaction_level',\n","                            'interpretation_level',\n","                            'exploration_level']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mQ_ZUxoMxK0o"},"source":["MAX_SAMPLE = 100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0gyLdDQzxMMf"},"source":["def getContentMetricDictFromFile(data_filepath,gold_filepath,response_type):\n","    print(\"start\")\n","    parsed_conversations = parse_sampled_responses.getParsedConversations(data_filepath,response_type)[0:MAX_SAMPLE]\n","\n","    parsed_conversations_dict = metric_averages_or_ratios.getMetricDict(parsed_conversations,content_metric_names, content_metric_names_separated)\n","    \n","    return parsed_conversations_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3LcsUtF2xO4w"},"source":["gold_content_metrics = getContentMetricDictFromFile(gold_path,gold_path,'gold_response')\n","\n","empathy_dict = {}\n","counter = 0\n","for path in path_list:\n","    print(path)\n","    empathy_metrics = getContentMetricDictFromFile(f\"sampled_responses/policy-1/{path}.json\",gold_path,'sample0')\n","    empathy_dict[x[counter]] = empathy_metrics\n","    counter = counter + 1\n","print(empathy_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"msOAjB08xRjN"},"source":["df2 = pd.DataFrame(empathy_dict).transpose()\n","\n","nrows = 3\n","\n","plot_names = [i[0] for i in list(gold_content_metrics.items())]\n","\n","fig = make_subplots(rows=nrows, cols=1, subplot_titles=plot_names)\n","\n","for i in range(0,nrows):\n","    fig.append_trace(go.Scatter(\n","        x=list(df2.index),\n","        y=df2.iloc[:,i],\n","        name=plot_names[i],\n","        legendgroup = '1',\n","    ), row=(i+1), col=1)\n","    \n","fig.update_xaxes(title_text=\"training steps\", row=nrows, col=1)\n","fig.update_yaxes(title_text=\"average word count\", row=(2), col=1)\n","\n","\n","# add shapes\n","col_count = 1\n","for i in range(0,nrows):\n","    gold = list(gold_content_metrics.items())[i][1]\n","    fig.add_shape(go.layout.Shape(type=\"line\",\n","                                    x0=0,\n","                                    y0=gold,\n","                                    x1=200,\n","                                    y1=gold,\n","                                    line=dict(color='yellow', width=2),),\n","                  row=(i+1),\n","                  col=1)\n","    fig.append_trace(go.Scatter(\n","        showlegend = False,\n","        x=[210],\n","        y=[gold],\n","        text=[\"gold\"],\n","        mode=\"text\",\n","    ), row=(i+1), col=1)\n","    col_count = col_count+1\n","\n","\n","fig.update_layout(height=600, width=800, title_text=\"Word Count metrics vs KL calculated from \"+str(MAX_SAMPLE)+\" samples\")\n","fig.show()"],"execution_count":null,"outputs":[]}]}