{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"nlp.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1uNIL-AdQwXcskmmRrHUgpNiUw3chOsyX","authorship_tag":"ABX9TyPqSDFTdErp8k9OBRdeScJX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"USX1kVz5VdUW"},"source":["%tensorflow_version 1.13.1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uEmQ8U4CWnA-"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"APqQ_rCNZWxd"},"source":["%cd /content/drive/My Drive/nlp-2021-vda\n","!pwd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L6XTMyY8aEb9"},"source":["from scripts import pre_train_gpt2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x-lJ5GmIa8L_"},"source":["sess = pre_train_gpt2.start_tf_sess()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nlttl3BmWH8e"},"source":["#Parameters Descriptions\n","\n","# sess - Tensorflow session\n","# dataset - path where the data is located\n","# steps - for how many steps do you wanna train the model\n","# model_name - Initial GPT model name i.e 124M or 335M or 775M\n","# model_dir - path where the initial GPT model is stored\n","# batch_size - Batch Size\n","# learning_rate - Learning Rate\n","# accumulate gradients - Accumulate gradients across N minibatches\n","# input_maxlen - maximum length of input tokens\n","# history_len - How many previous dialogues should be used in the context\n","# restore_from - Either \"latest\", \"fresh\", or a path to a checkpoint file\n","# run_name - Run id. Name of subdirectory in checkpoint/\n","# checkpoint_dir - path where the checkpoints should be stored or located\n","# multi_gpu - set True, if you have multiple GPUs\n","# save_every - Write a checkpoint every N steps\n","# print_every - Print stats every N steps\n","# optimizer - which optimizer to use\n","# overwrite - Set true, if you wanna overwrite previous checkpoints\n","\n","pre_train_gpt2.finetune(sess,\n","             dataset='datasets/empatheticdialogues/train.csv',\n","             steps=-1,\n","             model_name='124M',\n","             model_dir='gpt-2/models',\n","             batch_size=1,\n","             learning_rate=0.0001,\n","             accumulate_gradients=5,\n","             input_maxlen=100,\n","             history_len=4,\n","             restore_from='latest',\n","             run_name='run1',\n","             checkpoint_dir='checkpoint',\n","             multi_gpu=False,\n","             save_every=1000,\n","             print_every=1,\n","             max_checkpoints=1,\n","             optimizer='adam',\n","             overwrite=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2t7nFqtjaRKg"},"source":[""],"execution_count":null,"outputs":[]}]}